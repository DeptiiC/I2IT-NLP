{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization"
      ],
      "metadata": {
        "id": "_hGAwQF4SbQu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What is Tokenization?**\n",
        "\n",
        "Tokenization is a process of converting raw data into a useful data string. Tokenization is used in NLP for splitting paragraphs and sentences into smaller chunks that can be more easily assigned meaning.\n",
        "\n",
        "Tokenization can be done to either at word level or sentence level. If the text is split into words it is called word tokenization and the separation done for sentences is called sentence tokenization.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KY_kGyu3nZoM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why is Tokenization required?**\n",
        "\n",
        "In tokenization process unstructured data and natural language text is broken into chunks of information that can be understood by machine.\n",
        "\n",
        "Tokenization converts an unstructured string (text document) into a numerical data structure suitable for machine learning. This allows the machines to understand each of the words by themselves, as well as how they function in the larger text. This is especially important for larger amounts of text as it allows the machine to count the frequencies of certain words as well as where they frequently appear. \n",
        "\n",
        "Tokenization is the first crucial step of the NLP process as it converts sentences into understandable bits of data for the program to work with. Without a proper / correct tokenization, the NLP process can quickly devolve into a chaotic task. "
      ],
      "metadata": {
        "id": "KQHAVt3WtosW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Challenges of Tokenization**\n",
        "\n",
        "\n",
        "*   Dealing with segment words when spaces or punctuation marks define the boundaries of the word. For example: donÃ¢â‚¬â„¢t\n",
        "*   Dealing with symbols that might change the meaning of the word significantly. For example: â‚¹100 vs 100\n",
        "*   Contractions such as â€˜youâ€™reâ€™ and â€˜Iâ€™mâ€™ should be properly broken down into their respective parts. An improper tokenization of the sentence can lead to misunderstandings later in the NLP process.\n",
        "*   In languages like English or French we can separate words by using white spaces, or punctuation marks to define the boundary of the sentences. But this method is not applicable for symbol based languages like Chinese, Japanese, Korean Thai, Hindi, Urdu, Tamil, and others. Hence a common tokenization tool that combines all languages is needed.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MnV3VE_Dv-pZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Types of Tokenization**\n",
        "1.   **Word Tokenization**\n",
        "*   Most common way of tokenization, uses natural breaks, like pauses in speech or spaces in text, and splits the data into its respective words using delimiters (characters like â€˜,â€™ or â€˜;â€™ or â€˜â€œ,â€â€™).  \n",
        "*   Word tokenizationâ€™s accuracy is based on the vocabulary it is trained with. Unknown words or Out Of Vocabulary (OOV) words cannot be tokenized.\n",
        "\n",
        "2.   **White Space Tokenization**\n",
        "*   Simplest technique, Uses white spaces as basis of splitting.\n",
        "*   Works well for languages in which the white space breaks apart the sentence into meaningful words.\n",
        "\n",
        "3.   **Rule Based Tokenization**\n",
        "*   Uses a set of rules that are created for the specific problem.\n",
        "*   Rules are usually based on grammar for particular language or problem.\n",
        "\n",
        "4.   **Regular Expression Tokenizer**\n",
        "*   Type of Rule based tokenizer\n",
        "*   Uses regular expression to control the tokenization of text into tokens.\n",
        "\n",
        "5.   **Penn Treebank Tokenizer**\n",
        "*   Penn Treebank is a corpus maintained by the University of Pennsylvania containing over four million and eight hundred thousand annotated words in it, all corrected by humans\n",
        "*   Uses regular expressions to tokenize text as in Penn Treebank"
      ],
      "metadata": {
        "id": "jcrl5RrQ0c5O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Perform tokenization (Whitespace, Punctuation-based, Treebank, Tweet, Multi-Word Expression - MWE) using NLTK library."
      ],
      "metadata": {
        "id": "Pkw1vgBtGohs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lz8UIzOitaG2",
        "outputId": "e29fbd05-bf53-47d3-c456-843aa1821b39"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (3.7)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from nltk) (2022.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from nltk) (4.64.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Whitespace based Tokenization**"
      ],
      "metadata": {
        "id": "tV1DmxuXIA8W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Syntax : tokenize.WhitespaceTokenizer()\n",
        "\n",
        "Return : Return the tokens from a string"
      ],
      "metadata": {
        "id": "3ninuLRfIWmA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MXMZ4mzceDyq",
        "outputId": "9c2379a8-53ba-45bb-c95d-f36bcc4613b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Original string:\n",
            "Welcome to the I2IT-NLP Page. \n",
            " Good Morning \t\n",
            "\n",
            "Splitting using whitespece into separate tokens:\n",
            "['Welcome', 'to', 'the', 'I2IT-NLP', 'Page.', 'Good', 'Morning']\n"
          ]
        }
      ],
      "source": [
        "# import WhitespaceTokenizer() method from nltk\n",
        "from nltk.tokenize import WhitespaceTokenizer\n",
        "     \n",
        "# Create a reference variable for Class WhitespaceTokenizer\n",
        "wt = WhitespaceTokenizer()\n",
        "\n",
        "# Create a string input\n",
        "text = \"Welcome to the I2IT-NLP Page. \\n Good Morning \\t\"\n",
        "print(\"\\nOriginal string:\")\n",
        "print(text)     \n",
        "# Use tokenize method\n",
        "tokenized_text = wt.tokenize(text)\n",
        "print(\"\\nSplitting using whitespece into separate tokens:\")\n",
        "print(tokenized_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Punctuation-based tokenizer**"
      ],
      "metadata": {
        "id": "xheToBgAOHAf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import WordPunctTokenizer\n",
        "text = \"Welcome to the I2IT-NLP Page. \\n Good Morning \\t\"\n",
        "print(\"\\nOriginal string:\")\n",
        "print(text)\n",
        "result = WordPunctTokenizer().tokenize(text)\n",
        "print(\"\\nSplit all punctuation into separate tokens:\")\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yc66L2djPJw5",
        "outputId": "396e7994-a882-44af-aa1e-977a4d134545"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Original string:\n",
            "Welcome to the I2IT-NLP Page. \n",
            " Good Morning \t\n",
            "\n",
            "Split all punctuation into separate tokens:\n",
            "['Welcome', 'to', 'the', 'I2IT', '-', 'NLP', 'Page', '.', 'Good', 'Morning']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Treebank Tokenizer**"
      ],
      "metadata": {
        "id": "EZXRMV3oRf7b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer\n",
        " \n",
        "tokenizer = TreebankWordTokenizer()\n",
        "tokenizer.tokenize(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "akr3mMXyR4qk",
        "outputId": "5371a447-e568-4e90-a596-9b8b8cad83d8"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Welcome', 'to', 'the', 'I2IT-NLP', 'Page.', 'Good', 'Morning']"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tweet Tokenizer**\n",
        "\n",
        "When we want to apply tokenization in text data like tweets, the tokenizers mentioned above canâ€™t produce practical tokens. Through this issue, NLTK has a rule based tokenizer special for tweets. We can split emojis into different words if we need them for tasks like sentiment analysis."
      ],
      "metadata": {
        "id": "9UUPDg6WSSFF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "tweet_tokenize = TweetTokenizer()\n",
        "sample_tweet = \"Who is your favourite cryptocurrency influencer? ðŸ—£ðŸ† Tag them below! ðŸ‘‡\"\n",
        "print(tweet_tokenize.tokenize(sample_tweet))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fBg7Ks0OSVbv",
        "outputId": "9f712d9f-15e4-4d52-88db-d4a32c3194c5"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Who', 'is', 'your', 'favourite', 'cryptocurrency', 'influencer', '?', 'ðŸ—£', 'ðŸ†', 'Tag', 'them', 'below', '!', 'ðŸ‘‡']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Multi-Word Expression Tokenizer**\n",
        "\n",
        "A MWETokenizer takes a string which has already been divided into tokens and retokenizes it, merging multi-word expressions into single tokens, using a lexicon of MWEs.\n",
        "\n",
        "The multi-word expression tokenizer is a rule-based, â€œadd-onâ€ tokenizer offered by NLTK. Once the text has been tokenized by a tokenizer of choice, some tokens can be re-grouped into multi-word expressions.\n",
        "\n",
        "For example, the name Steven Spielberg is combined into a single token instead of being broken into two tokens. This tokenizer is very flexible since it is agnostic of the base tokenizer that was used to generate the tokens."
      ],
      "metadata": {
        "id": "zTJrhPFAMisl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import MWETokenizer() method from nltk\n",
        "from nltk.tokenize import MWETokenizer\n",
        "#from nltk.tokenize import  word_tokenize\n",
        "\n",
        "# Create a reference variable for Class MWETokenizer\n",
        "tokenizer = MWETokenizer([('a', 'little'), ('a', 'little', 'bit'), ('a', 'lot')])\n",
        "\n",
        "tokenizer.add_mwe(('in', 'spite', 'of'))\n",
        "tokenizer.tokenize('In a little or a little bit or a lot in spite of'.split())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_mjT3cMjNYVW",
        "outputId": "19143200-48d8-4c7d-c582-395b6ae23166"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['In', 'a_little', 'or', 'a_little_bit', 'or', 'a_lot', 'in_spite_of']"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import MWETokenizer\n",
        "tokenizer = MWETokenizer()\n",
        "tokenizer.add_mwe(('Steven', 'Spielberg'))\n",
        "tokenizer.tokenize('Steven Spielberg is an American writer producer director '.split())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_LDNaecgRkol",
        "outputId": "4d3a36c1-444e-4ab1-9276-b4260814557e"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Steven_Spielberg', 'is', 'an', 'American', 'writer', 'producer', 'director']"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stemming"
      ],
      "metadata": {
        "id": "bJkQrfPQSSYY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stemming is a process of reducing inflectional words to their root form. It maps the word to a same stem even if the stem is not a valid word in the language.\n",
        "\n",
        "**Why is stemming required?**\n",
        "\n",
        "English language has several variants of a single term. The presence of these variances in a text corpus results in data redundancy when developing NLP or machine learning models. Such models may be ineffective.\n",
        "\n",
        "To build a robust model, it is essential to normalize text by removing repetition and transforming words to their base form through stemming."
      ],
      "metadata": {
        "id": "CCycJhtjSW_U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Types of Stemmers in NLTK**\n",
        "*   Porter Stemmer\n",
        "*   Snowball Stemmer\n",
        "*   Lancaster Stemmer\n",
        "*   Regexp Stemmer"
      ],
      "metadata": {
        "id": "s7K_7kEgV_Rn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Porter Stemmer** \n",
        "\n",
        "It is a type of stemmer which is mainly known for Data Mining and Information Retrieval. As its applications are limited to the English language only. It is based on the idea that the suffixes in the English language are made up of a combination of smaller and simpler suffixes, it is also majorly known for its simplicity and speed. The advantage is, it produces the best output from other stemmers and has less error rate."
      ],
      "metadata": {
        "id": "3XZaoCN_UkxD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "ps = PorterStemmer()\n",
        "example_words =[\"connector\",\"connection\",\"connects\",\"connecting\",\"connected\"]\n",
        "         \n",
        "#Next, we can easily stem by doing something like:\n",
        "for w in example_words:\n",
        "  print(ps.stem(w))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HWlwhtKQEUCF",
        "outputId": "7b51c728-0bca-4c67-bd1e-bdb1a989ab73"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "connector\n",
            "connect\n",
            "connect\n",
            "connect\n",
            "connect\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Snowball Stemmer**"
      ],
      "metadata": {
        "id": "DcVk7qyKXHRI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "snowball = SnowballStemmer(language='english')\n",
        "words = ['generous','generate','generously','generation']\n",
        "for word in words:\n",
        "    print(word,\"--->\",snowball.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAGN75s2XLbE",
        "outputId": "602c63ab-3a16-4def-9804-c17f5379748c"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "generous ---> generous\n",
            "generate ---> generat\n",
            "generously ---> generous\n",
            "generation ---> generat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lancaster Stemmer**"
      ],
      "metadata": {
        "id": "sACRtj5tXS43"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import LancasterStemmer\n",
        "lancaster = LancasterStemmer()\n",
        "words = ['eating','eats','eaten','puts','putting']\n",
        "for word in words:\n",
        "    print(word,\"--->\",lancaster.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AjMhLJiMXVfY",
        "outputId": "2d80177c-9250-4551-a21e-eef7e1e78edb"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eating ---> eat\n",
            "eats ---> eat\n",
            "eaten ---> eat\n",
            "puts ---> put\n",
            "putting ---> put\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Regex Stemmer**"
      ],
      "metadata": {
        "id": "ivQXpxIhXZXt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import RegexpStemmer\n",
        "regexp = RegexpStemmer('ing$|s$|e$|able$', min=4)\n",
        "words = ['mass','was','bee','computer','advisable']\n",
        "for word in words:\n",
        "    print(word,\"--->\",regexp.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9LOieGQLXblB",
        "outputId": "16154fb1-eb3e-456e-9479-4515468e5c8b"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mass ---> mas\n",
            "was ---> was\n",
            "bee ---> bee\n",
            "computer ---> computer\n",
            "advisable ---> advis\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Porter Vs Snowball Vs Lancaster Vs Regex Stemmers**\n",
        "\n"
      ],
      "metadata": {
        "id": "ESqsRT3KXhb-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer, SnowballStemmer, LancasterStemmer, RegexpStemmer\n",
        "porter = PorterStemmer()\n",
        "lancaster = LancasterStemmer()\n",
        "snowball = SnowballStemmer(language='english')\n",
        "regexp = RegexpStemmer('ing$|s$|e$|able$', min=4)\n",
        "word_list = ['generous','generate','generously','generation']\n",
        "print(\"{0:20}{1:20}{2:20}{3:30}{4:40}\".format(\"Word\",\"Porter Stemmer\",\"Snowball Stemmer\",\"Lancaster Stemmer\",'Regexp Stemmer'))\n",
        "for word in word_list:\n",
        "    print(\"{0:20}{1:20}{2:20}{3:30}{4:40}\".format(word,porter.stem(word),snowball.stem(word),lancaster.stem(word),regexp.stem(word)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "roEhJiRqXnXw",
        "outputId": "083ba09b-94df-46c5-814c-398e32c0a3f2"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word                Porter Stemmer      Snowball Stemmer    Lancaster Stemmer             Regexp Stemmer                          \n",
            "generous            gener               generous            gen                           generou                                 \n",
            "generate            gener               generat             gen                           generat                                 \n",
            "generously          gener               generous            gen                           generously                              \n",
            "generation          gener               generat             gen                           generation                              \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lemmatization"
      ],
      "metadata": {
        "id": "p3FpzRvSYL7Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Use any technique for lemmatization.**"
      ],
      "metadata": {
        "id": "45oT2GO-YOwF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lemmatization**, unlike stemming reduces the inflected words properly ensuring that the root word belongs to the language.\n",
        "\n",
        "Lemmatization is the grouping together of different forms of the same word. In search queries, lemmatization allows end users to query any version of a base word and get relevant results. Because search engine algorithms use lemmatization, the user is free to query any inflectional form of a word and get relevant results. For example, if the user queries the plural form of a word (e.g., routers), the search engine knows to also return relevant content that uses the singular form of the same word (router).\n",
        "\n",
        "Lemmatization is extremely important because it is far more accurate than stemming. This brings great value when working with a chatbot where it is crucial to understand the meaning of a userâ€™s messages.\n",
        "\n",
        "The major disadvantage to lemmatization algorithms, however, is that they are much slower than stemming algorithms."
      ],
      "metadata": {
        "id": "F11R27q-YRdq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using NLTK Library for Lemmatization**"
      ],
      "metadata": {
        "id": "qhj7j-DfaQAm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J4cSj__tZ11R",
        "outputId": "63584a1b-e141-4b3a-ced7-15ce17949230"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "sentence = \"He was running and eating at same time. He has bad habit of swimming after playing long hours in the Sun.\"\n",
        "punctuations=\"?:!.,;\"\n",
        "sentence_words = nltk.word_tokenize(sentence)\n",
        "for word in sentence_words:\n",
        "    if word in punctuations:\n",
        "        sentence_words.remove(word)\n",
        "\n",
        "sentence_words\n",
        "print(\"{0:20}{1:20}\".format(\"Word\",\"Lemma\"))\n",
        "for word in sentence_words:\n",
        "    print (\"{0:20}{1:20}\".format(word,wordnet_lemmatizer.lemmatize(word)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9yahR4IcZv40",
        "outputId": "e6fa395f-12f5-4e2f-88a9-3dba7de0740b"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word                Lemma               \n",
            "He                  He                  \n",
            "was                 wa                  \n",
            "running             running             \n",
            "and                 and                 \n",
            "eating              eating              \n",
            "at                  at                  \n",
            "same                same                \n",
            "time                time                \n",
            "He                  He                  \n",
            "has                 ha                  \n",
            "bad                 bad                 \n",
            "habit               habit               \n",
            "of                  of                  \n",
            "swimming            swimming            \n",
            "after               after               \n",
            "playing             playing             \n",
            "long                long                \n",
            "hours               hour                \n",
            "in                  in                  \n",
            "the                 the                 \n",
            "Sun                 Sun                 \n"
          ]
        }
      ]
    }
  ]
}